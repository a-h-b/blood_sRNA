#!/bin/bash -l

#######################################################################################################################
# --------------------------------------------------- small RNA Analysis ----------------------------------------------
# ------------------------------ Anna Heintz-Buschart's version for pre-processing, March 2017 ------------------------
#  based on the BEar analysis script adapted by Anke Katrin Wienecke-Baldacchino in July 2016 from scripts by D. Yusuf
#######################################################################################################################

# this script documents the initial steps of the sRNA sequence analyis
# all necessary directories are generated on the fly
# besides some scripts needed to run this pipeline, all other scripts needed are generated on the fly
# note the setting of absolute paths in lines labeld with #absolutePaths

# --------------------------------  dependencies:
# - bedtools
# - fastx-toolkit
# - samtools
# - parallel
# - blast (makeblastdb, blastn)
# - FastQC
# - numpy (for cmc.py --> weighted count calculation)

#on University of Luxembourgs GAIA cluster:
module use $RESIF_ROOTINSTALL/lcsb/modules/all
module load bio/BEDTools/2.23.0-goolf-1.4.10
module load bio/FASTX-Toolkit/0.0.14-goolf-1.4.10
module load bio/FastQC/0.11.2
module load tools/parallel/20130122-goolf-1.4.10
module load bio/SAMtools/1.2-ictce-5.3.0
module use $RESIF_ROOTINSTALL/core/modules/all
module load math/numpy/1.8.0-ictce-5.3.0-Python-2.7.5
module load bio/BLAST+/2.2.28-goolf-1.4.10

# --------------------------------  scripts/files needed to run the pipeline:
scriptsdir=absolute_path_to_scripts #absolutePaths
# - clean_lib.py
# - novoindex/novoalign (commercial)


# --------------------------------  scripts/files generated by the pipeline:
# - fastx_clipper_parallel.sh / fastx_clipper_parallel.log
# - fastq_quality_trimmer.log
# - fastq_quality_filter.log
# - fastx_collapser.log
# - makeAlignmentHuman.sh / makeAlignmentHuman.log


# --------------------------------  directories generated
# ./fastq
# ./QC
# ./cln.adapt.1
# ./cln.adapt.2
# ./cln.filt
# ./cln.trim
# ./aln_hg38
# ./uniq_fasta
# ./unmapped_human

#-----------------------------------------------------------------------------------------------------------------------
#                                                  START PIPELINE
#-----------------------------------------------------------------------------------------------------------------------
id=${PWD##*/}
sample=`grep $id ../../plasma.ids | cut -f 1`

##directory with the original fastq-files
rawDirectory=/absolute_path_to_raw_data #absolutePaths
rawData="$rawDirectory/$sample" 

############################################## PREPARE INPUT DATA ######################################################

mkdir fastq
zcat $rawData/*fastq.gz >fastq/$id.fastq

######################################################## QC ############################################################

# --------------------------------------------- initial FastQC check ---------------------------------------------------
mkdir QC
parallel --gnu fastqc --extract -o QC {} ::: fastq/*.fastq

# --------------------------------------------- cut adapters and check -------------------------------------------------
mkdir cln.adapt.1
cd cln.adapt.1
#primer_contamination.csv generated, extracted from fastqc report
$scriptsdir/clean_lib.py -t ../QC/*/fastqc_data.txt
#take top sequence
##LINUX: 
grep primer_seq primer_contamination.csv | sed 's/\t/\n/g'| head -2 | tail -1 > contaminant_top
#generate script extracting the top enriched sequence and pipe it to fastx_collapser
more contaminant_top | awk '{print "parallel --gnu fastx_clipper -Q33 -a "$1"  -l 14 -v -i {} -o {/.}.fq ::: ../fastq/*.fastq"}' > fastx_clipper_parallel.sh
sh fastx_clipper_parallel.sh > fastx_clipper_parallel.log
curfastq=cln.adapt.1
cd ..
# original fastq-files can be zipped to save disk space
gzip fastq/*.fastq

# --------------------------------------------- second FastQC check ----------------------------------------------------

#make second fastQC run to check whether still adapter sequences or any other enriched sequence is in fastq-files
mkdir -p QC/1
parallel --gnu fastqc --extract -o QC/1 {} ::: $curfastq/*.fq

mkdir cln.adapt.2
cd cln.adapt.2

#primer_contamination.csv generated, extracted from fastqc report
$scriptsdir/clean_lib.py -t ../QC/1/*/fastqc_data.txt
#take top sequence
grep primer_seq primer_contamination.csv | sed 's/\t/\n/g'| head -2 | tail -1 > contaminant_top
stop=`grep primer_seq contaminant_top`
if [ -z $stop ] 
then
    more contaminant_top | awk '{print "parallel --gnu fastx_clipper -Q33 -a "$1"  -l 14 -v -i {} -o {/.}.fq ::: ../'$curfastq'/*.fq"}' > fastx_clipper_parallel.sh
    sh fastx_clipper_parallel.sh > fastx_clipper_parallel.log

    curfastq=cln.adapt.2
fi
cd ..

# ---------------------------------- trim clean sequences based on base quality ----------------------------------------

mkdir -p cln.trim
#Following the visualization of length distribution of sequences in all fastq files produced by fastqc, a length threshold of 14 was determined.
# A quality threshold of 25 phred score was used.
parallel --gnu fastq_quality_trimmer -Q33 -v -t 25 -l 14 -i {} -o cln.trim/{/.}_trim.fq ::: $curfastq/*.fq >> fastq_quality_trimmer.log

# clipped fastq-files from 1st QC can be zipped to save disk space
gzip $curfastq/*.fq
# ---------------------------------------------- third FastQC check ----------------------------------------------------

#check quality of trimmed sequences
curfastq=cln.trim
mkdir -p QC/trim
parallel --gnu fastqc --extract -o QC/trim {} ::: $curfastq/*.fq

# ------------------------------------- filter reads based on base quality ---------------------------------------------

mkdir -p cln.filt
#Sequences were filtered with a phred score of 25 throughout the read.
parallel --gnu fastq_quality_filter -Q33 -v -q 25 -p 100 -i {} -o cln.filt/{/.}.fq ::: $curfastq/*.fq >> fastq_quality_filter.log

# trimmed fastq-files can be zipped to save disk space
gzip $curfastq/*.fq
# ---------------------------------------------- fourth FastQC check ----------------------------------------------------

#check quality of quality filtered sequences
curfastq=cln.filt
mkdir -p QC/filt
parallel --gnu fastqc --extract -o QC/filt {} ::: $curfastq/*.fq

# ------------------------ merge all repeated sequences to one representative -------------------------------------------

#This step is to merge all repeated sequences into unique tags maintaining the counts.
# Since we follow strict preprocessing, during mapping we use the fasta files generated in this step.
# No quality information is needed anymore, therefore fasta format ok.
mkdir -p uniq_fasta
parallel --gnu fastx_collapser -v -i {} -o uniq_fasta/{/.}.fa ::: $curfastq/*.fq >> fastx_collapser.log

# filtered fastq-files can be zipped to save disk space
gzip $curfastq/*.fq

##################################################### ALIGNMENT ########################################################


# ------------------------------------------ align reads to human genome -------------------------------------------------

# attention: absolute PATH to data dir!!!
mkdir -p aln_hg38
cd aln_hg38

ln -s $scriptsdir/novoalign 
ls -1 ../uniq_fasta/ | awk '{print "./novoalign -d /absolute_path_to_hg38.novoindex -f  ../uniq_fasta/"$1" -t 60 -h -1 -l 6 -o SAM -r ALL -e 51 > "$1"_hg38.sam"}' >  makeAlignmentHuman.sh #absolutePaths
sh makeAlignmentHuman.sh 2>> makeAlignmentHuman.log
cd ..

# -------------------------------- Filter unmapped reads, just keep mapped reads ---------------------------------------

mkdir -p mapped_human
cd mapped_human
parallel --gnu samtools view -h -S -F 4 {} '>' {/.}.sam ::: ../aln_hg38/*.sam
cd ../

# --------------------------------- extract unmapped reads -------------------------------------------------------------

mkdir -p unmapped_human
cd unmapped_human
ls -1 ../aln_hg38/ | grep ".sam" | awk '{print "samtools view -f4 ../aln_hg38/"$1" | awk -F\"\t\" '"'"'{print \">\"$1\"\\n\"$10}'"'"' > "$1"_uM.fa"}' | sed 's/.sam_/_/g' > extractUnmappedReadsHuman.sh 
ls -1 ../aln_hg38/ | grep ".sam" | awk '{print "samtools view -f4 ../aln_hg38/"$1" > "$1"_uM.sam"}' >> extractUnmappedReadsHuman.sh 
sh extractUnmappedReadsHuman.sh
cd ..


